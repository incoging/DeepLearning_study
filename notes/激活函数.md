## 激活函数

### 1.ReLU

图像：

![微信截图_20180804111407](.\激活函数.assets/微信截图_20180804111407.png)





### 2. Leaky ReLU

动机：

1. 防止反向传播时有0梯度。

效果：

其实leaky ReLU相比于ReLU在结果的准确率上面是没有太大影响的。paper: Rectifier nonlinearities improve neural network acoustic models. 

### 3. PReLU

**函数图像**：



![微信截图_20180804111249](.\激活函数.assets/微信截图_20180804111249.png)

**公式定义**：

![微信截图_20180804111818](X:/jet_python/machineLearning/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180804111818.png)

其中，$${y_i}$$表示非线性激活函数$$f$$在第$$i$$个通道的输入；$${a_i}$$的下标$$i$$表示在每个通道允许有不同的非线性激活。$${a_i}$$是可学习参数，论文中初始化为0.25.

这个公式也可以表达为：

![微信截图_20180804112755](X:/jet_python/machineLearning/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180804112755.png)

也就是说对于每一个通道有一个$${a_i}$$，但是，同样为了防止过拟合，也可以每一层使用同一个$$a$$变量，而不是每个通道：

![微信截图_20180804112944](X:/jet_python/machineLearning/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0.assets/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20180804112944.png)

**优化**：

对$${a_i}$$在某一层的优化有：

![微信截图_20180804114046](.\激活函数.assets/微信截图_20180804114046.png)

其中：$$\varepsilon $$表示目标函数，$$\frac{{\partial \varepsilon }}{{\partial f({y_i})}}$$是从深层传播的梯度，

![微信截图_20180804114638](.\激活函数.assets/微信截图_20180804114638-1533354430210.png)

![微信截图_20180804115109](.\激活函数.assets/微信截图_20180804115109.png)







